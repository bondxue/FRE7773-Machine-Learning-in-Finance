{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Tuning Examples\n",
    "**Make sure you have activated the correct python envorinment**\n",
    "\n",
    "+ DNN = Dense Neural Network\n",
    "+ Using the keras Sequential API and MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "np.random.seed(42)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.initializers import RandomNormal, he_normal\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "MNIST is a dataset of 60,000 28x28 grayscale images of handwritten digits, along with a test set of 10,000 images.\n",
    "\n",
    "Load the MNIST data using keras. The first time the data are downloaded and cached.  \n",
    "Subsequent times the data are loaded from the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True values: [4 0 0 8 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the date and split into training/testing sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# plot a few digits and print the true values\n",
    "idigits = np.random.randint(0, len(y_train), 5)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "for i in range(len(idigits)):\n",
    "    ax = fig.add_subplot(1, len(idigits), i + 1)\n",
    "    ax.imshow(x_train[idigits[i]], cmap = plt.cm.binary, interpolation=\"nearest\")\n",
    "    ax.axis(\"off\")\n",
    "print('True values: {}'.format(y_train[idigits]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape, cast, normalize inputs\n",
    "* Reshape the data: 28x28 -> 784  \n",
    "* Cast the values int8 -> float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_train = len(x_train)  # 60000\n",
    "n_test = len(x_test)  # 10000\n",
    "input_dim = 28 * 28\n",
    "\n",
    "x_train = x_train.reshape(n_train, input_dim)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.reshape(n_test, input_dim)\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the target values vector to binary class matrix\n",
    "y_train is a vector of size 60,000.  \n",
    "It gets mapped to a 60,000 x 10 matrix of 0s and 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets before: \n",
      "[5 0 4 1 9]\n",
      "Targets after: \n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Targets before: \\n{}\".format(y_train[:5]))\n",
    "ybm_train = to_categorical(y_train, n_classes)\n",
    "ybm_test = to_categorical(y_test, n_classes)\n",
    "print(\"Targets after: \\n{}\".format(ybm_train[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the common network parameters\n",
    "# Number of hidden layers and units per layer\n",
    "hidden = [100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn1 = Sequential()\n",
    "nn1.add(Dense(hidden[0], input_dim=input_dim, activation='relu'))\n",
    "for i in range(len(hidden) - 1):\n",
    "    nn1.add(Dense(hidden[i + 1], activation='relu'))\n",
    "nn1.add(Dense(n_classes, activation='softmax'))\n",
    "nn1.summary()\n",
    "\n",
    "# the number of parameters is: n_classes x (input_dim + 1)\n",
    "# each node has its own bias (therefore +1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/40\n",
      "54000/54000 [==============================] - 1s 20us/step - loss: 2.2215 - acc: 0.2429 - val_loss: 2.0862 - val_acc: 0.3863\n",
      "Epoch 2/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.9558 - acc: 0.4678 - val_loss: 1.7983 - val_acc: 0.5668\n",
      "Epoch 3/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.6681 - acc: 0.6067 - val_loss: 1.4877 - val_acc: 0.6947\n",
      "Epoch 4/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.3793 - acc: 0.6979 - val_loss: 1.1990 - val_acc: 0.7638\n",
      "Epoch 5/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.1345 - acc: 0.7517 - val_loss: 0.9739 - val_acc: 0.8105\n",
      "Epoch 6/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.9540 - acc: 0.7844 - val_loss: 0.8150 - val_acc: 0.8348\n",
      "Epoch 7/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.8270 - acc: 0.8062 - val_loss: 0.7036 - val_acc: 0.8550\n",
      "Epoch 8/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.7364 - acc: 0.8218 - val_loss: 0.6238 - val_acc: 0.8658\n",
      "Epoch 9/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.6699 - acc: 0.8345 - val_loss: 0.5647 - val_acc: 0.8740\n",
      "Epoch 10/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.6194 - acc: 0.8441 - val_loss: 0.5192 - val_acc: 0.8812\n",
      "Epoch 11/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.5797 - acc: 0.8522 - val_loss: 0.4837 - val_acc: 0.8880\n",
      "Epoch 12/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.5476 - acc: 0.8587 - val_loss: 0.4551 - val_acc: 0.8920\n",
      "Epoch 13/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.5213 - acc: 0.8645 - val_loss: 0.4315 - val_acc: 0.8967\n",
      "Epoch 14/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.4991 - acc: 0.8686 - val_loss: 0.4120 - val_acc: 0.9002\n",
      "Epoch 15/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.4803 - acc: 0.8721 - val_loss: 0.3954 - val_acc: 0.9027\n",
      "Epoch 16/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.4641 - acc: 0.8752 - val_loss: 0.3810 - val_acc: 0.9050\n",
      "Epoch 17/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.4499 - acc: 0.8777 - val_loss: 0.3689 - val_acc: 0.9053\n",
      "Epoch 18/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.4374 - acc: 0.8804 - val_loss: 0.3580 - val_acc: 0.9078\n",
      "Epoch 19/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.4264 - acc: 0.8828 - val_loss: 0.3483 - val_acc: 0.9087\n",
      "Epoch 20/40\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.4165 - acc: 0.8847 - val_loss: 0.3403 - val_acc: 0.9118\n",
      "Epoch 21/40\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.4076 - acc: 0.8867 - val_loss: 0.3324 - val_acc: 0.9127\n",
      "Epoch 22/40\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.3995 - acc: 0.8888 - val_loss: 0.3254 - val_acc: 0.9145\n",
      "Epoch 23/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3921 - acc: 0.8903 - val_loss: 0.3195 - val_acc: 0.9148\n",
      "Epoch 24/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3854 - acc: 0.8924 - val_loss: 0.3137 - val_acc: 0.9163\n",
      "Epoch 25/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3791 - acc: 0.8936 - val_loss: 0.3084 - val_acc: 0.9180\n",
      "Epoch 26/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3732 - acc: 0.8953 - val_loss: 0.3040 - val_acc: 0.9190\n",
      "Epoch 27/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3679 - acc: 0.8966 - val_loss: 0.2993 - val_acc: 0.9203\n",
      "Epoch 28/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3629 - acc: 0.8980 - val_loss: 0.2951 - val_acc: 0.9198\n",
      "Epoch 29/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3582 - acc: 0.8989 - val_loss: 0.2912 - val_acc: 0.9220\n",
      "Epoch 30/40\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.3537 - acc: 0.9001 - val_loss: 0.2875 - val_acc: 0.9240\n",
      "Epoch 31/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3495 - acc: 0.9009 - val_loss: 0.2841 - val_acc: 0.9242\n",
      "Epoch 32/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3454 - acc: 0.9021 - val_loss: 0.2809 - val_acc: 0.9252\n",
      "Epoch 33/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3417 - acc: 0.9030 - val_loss: 0.2778 - val_acc: 0.9258\n",
      "Epoch 34/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3380 - acc: 0.9042 - val_loss: 0.2749 - val_acc: 0.9262\n",
      "Epoch 35/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3345 - acc: 0.9049 - val_loss: 0.2718 - val_acc: 0.9273\n",
      "Epoch 36/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3313 - acc: 0.9060 - val_loss: 0.2694 - val_acc: 0.9275\n",
      "Epoch 37/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3281 - acc: 0.9064 - val_loss: 0.2670 - val_acc: 0.9275\n",
      "Epoch 38/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3250 - acc: 0.9074 - val_loss: 0.2645 - val_acc: 0.9287\n",
      "Epoch 39/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3221 - acc: 0.9079 - val_loss: 0.2623 - val_acc: 0.9282\n",
      "Epoch 40/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3193 - acc: 0.9088 - val_loss: 0.2600 - val_acc: 0.9295\n",
      "Default DNN\n",
      "------------------------\n",
      "Test loss score: 0.2985\n",
      "Test accuracy:   0.9143\n"
     ]
    }
   ],
   "source": [
    "nn1.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 1000\n",
    "n_epochs = 40\n",
    "val_split = 0.1 \n",
    "# create an early stopping callback\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "nn1_hist = nn1.fit(x_train, ybm_train, batch_size=batch_size, epochs=n_epochs, validation_split=val_split, callbacks=[es], verbose=1)\n",
    "val_score = nn1.evaluate(x_test, ybm_test, verbose=0)\n",
    "\n",
    "print('Default DNN')\n",
    "print('------------------------')\n",
    "print('Test loss score: {0:.4f}'.format(val_score[0]))\n",
    "print('Test accuracy:   {0:.4f}'.format(val_score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN with recommended settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Dropout rate\n",
    "drop_rate = 0.4\n",
    "# Create the initializer\n",
    "# ini = RandomNormal()\n",
    "ini = he_normal()\n",
    "\n",
    "nn2 = Sequential()\n",
    "nn2.add(Dense(hidden[0], input_dim=input_dim, activation='elu', kernel_initializer=ini))\n",
    "nn2.add(Dropout(drop_rate))\n",
    "for i in range(len(hidden) - 1):\n",
    "    nn2.add(Dense(hidden[i + 1], activation='elu', kernel_initializer=ini))\n",
    "    nn2.add(Dropout(drop_rate))    \n",
    "nn2.add(Dense(n_classes, activation='softmax', kernel_initializer=ini))\n",
    "nn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/40\n",
      "54000/54000 [==============================] - 1s 26us/step - loss: 0.8116 - acc: 0.7426 - val_loss: 0.2613 - val_acc: 0.9297\n",
      "Epoch 2/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.4143 - acc: 0.8760 - val_loss: 0.2181 - val_acc: 0.9373\n",
      "Epoch 3/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.3620 - acc: 0.8930 - val_loss: 0.1962 - val_acc: 0.9435\n",
      "Epoch 4/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.3254 - acc: 0.9029 - val_loss: 0.1726 - val_acc: 0.9488\n",
      "Epoch 5/40\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.3009 - acc: 0.9111 - val_loss: 0.1612 - val_acc: 0.9548\n",
      "Epoch 6/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2801 - acc: 0.9164 - val_loss: 0.1472 - val_acc: 0.9587\n",
      "Epoch 7/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2625 - acc: 0.9217 - val_loss: 0.1354 - val_acc: 0.9613\n",
      "Epoch 8/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2443 - acc: 0.9255 - val_loss: 0.1309 - val_acc: 0.9633\n",
      "Epoch 9/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2367 - acc: 0.9278 - val_loss: 0.1196 - val_acc: 0.9645\n",
      "Epoch 10/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2260 - acc: 0.9314 - val_loss: 0.1172 - val_acc: 0.9667\n",
      "Epoch 11/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2147 - acc: 0.9341 - val_loss: 0.1125 - val_acc: 0.9685\n",
      "Epoch 12/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2091 - acc: 0.9364 - val_loss: 0.1077 - val_acc: 0.9693\n",
      "Epoch 13/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.1993 - acc: 0.9389 - val_loss: 0.1021 - val_acc: 0.9707\n",
      "Epoch 14/40\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.1925 - acc: 0.9405 - val_loss: 0.1003 - val_acc: 0.9708\n",
      "Epoch 15/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.1858 - acc: 0.9434 - val_loss: 0.1001 - val_acc: 0.9705\n",
      "Epoch 16/40\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.1811 - acc: 0.9443 - val_loss: 0.0934 - val_acc: 0.9733\n",
      "Epoch 17/40\n",
      "54000/54000 [==============================] - 2s 32us/step - loss: 0.1787 - acc: 0.9447 - val_loss: 0.0928 - val_acc: 0.9733\n",
      "Epoch 18/40\n",
      "54000/54000 [==============================] - 2s 31us/step - loss: 0.1723 - acc: 0.9469 - val_loss: 0.0899 - val_acc: 0.9732\n",
      "Epoch 19/40\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.1647 - acc: 0.9506 - val_loss: 0.0877 - val_acc: 0.9740\n",
      "Epoch 20/40\n",
      "54000/54000 [==============================] - 1s 22us/step - loss: 0.1613 - acc: 0.9495 - val_loss: 0.0893 - val_acc: 0.9737\n",
      "Epoch 21/40\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.1580 - acc: 0.9512 - val_loss: 0.0880 - val_acc: 0.9742\n",
      "Epoch 22/40\n",
      "54000/54000 [==============================] - 2s 31us/step - loss: 0.1549 - acc: 0.9523 - val_loss: 0.0846 - val_acc: 0.9738\n",
      "Epoch 23/40\n",
      "54000/54000 [==============================] - 2s 30us/step - loss: 0.1516 - acc: 0.9524 - val_loss: 0.0856 - val_acc: 0.9757\n",
      "Epoch 24/40\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.1484 - acc: 0.9534 - val_loss: 0.0838 - val_acc: 0.9752\n",
      "Epoch 25/40\n",
      "54000/54000 [==============================] - 1s 22us/step - loss: 0.1446 - acc: 0.9551 - val_loss: 0.0811 - val_acc: 0.9757\n",
      "Epoch 26/40\n",
      "54000/54000 [==============================] - 1s 21us/step - loss: 0.1435 - acc: 0.9558 - val_loss: 0.0802 - val_acc: 0.9763\n",
      "Epoch 27/40\n",
      "54000/54000 [==============================] - 1s 22us/step - loss: 0.1376 - acc: 0.9571 - val_loss: 0.0800 - val_acc: 0.9757\n",
      "Epoch 28/40\n",
      "54000/54000 [==============================] - 1s 22us/step - loss: 0.1367 - acc: 0.9573 - val_loss: 0.0770 - val_acc: 0.9767\n",
      "Epoch 29/40\n",
      "54000/54000 [==============================] - 1s 23us/step - loss: 0.1332 - acc: 0.9588 - val_loss: 0.0772 - val_acc: 0.9775\n",
      "Epoch 30/40\n",
      "54000/54000 [==============================] - 1s 21us/step - loss: 0.1334 - acc: 0.9580 - val_loss: 0.0775 - val_acc: 0.9788\n",
      "Epoch 31/40\n",
      "54000/54000 [==============================] - 1s 21us/step - loss: 0.1317 - acc: 0.9592 - val_loss: 0.0774 - val_acc: 0.9765\n",
      "3-Layer DNN\n",
      "------------------------\n",
      "Test loss score: 0.0868\n",
      "Test accuracy:   0.9738\n"
     ]
    }
   ],
   "source": [
    "# compile, fit and evaluate\n",
    "nn2.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size2 = 1000\n",
    "n_epochs2 = 40\n",
    "val_split2 = 0.1 \n",
    "# create an early stopping callback\n",
    "es2 = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "nn2_hist = nn2.fit(x_train, ybm_train, batch_size=batch_size2, epochs=n_epochs2, validation_split=val_split2, callbacks=[es2], verbose=1)\n",
    "val_score = nn2.evaluate(x_test, ybm_test, verbose=0)\n",
    "\n",
    "print('3-Layer DNN')\n",
    "print('------------------------')\n",
    "print('Test loss score: {0:.4f}'.format(val_score[0]))\n",
    "print('Test accuracy:   {0:.4f}'.format(val_score[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN predictions and true values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn2_pred = nn2.predict(x_test[:20])\n",
    "nn2_ypred = np.argmax(nn2_pred, axis=1)\n",
    "\n",
    "print('DNN predictions and true values')\n",
    "display(nn2_ypred.tolist())\n",
    "display(y_test[:20].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "\n",
    "We fit a multiclass logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1e-05, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=1000, multi_class='multinomial',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "lgm = LogisticRegression(C=1e-5, multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "print(lgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1e-05, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgm.fit(x_train,  y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "------------------------\n",
      "Test accuracy:   0.7482\n"
     ]
    }
   ],
   "source": [
    "y_pred = lgm.predict(x_test)\n",
    "lg_acc = accuracy_score(y_test, y_pred)\n",
    "print('Logistic Regression')\n",
    "print('------------------------')\n",
    "print('Test accuracy:   {0:.4f}'.format(lg_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

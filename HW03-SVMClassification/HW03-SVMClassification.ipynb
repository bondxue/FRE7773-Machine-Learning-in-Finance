{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "#### Assigned: 2019-03-07\n",
    "#### Due EOD: 2019-03-14\n",
    "\n",
    "Based on Lecture 6\n",
    "+ The student fills in the <...> fields.  \n",
    "+ The student can create as many new cells as necessary in the solution sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Useful LaTeX macros that save typing (click into cell to see them)*\n",
    "$\\def\\bx{\\boldsymbol{x}}$\n",
    "$\\def\\by{\\boldsymbol{y}}$\n",
    "$\\def\\bz{\\boldsymbol{z}}$\n",
    "$\\def\\bM{\\boldsymbol{M}}$\n",
    "$\\def\\bphi{\\boldsymbol{phi}}$\n",
    "$\\def\\bpsi{\\boldsymbol{psi}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Name:  Mengheng Xue\n",
    "#### NetID: mx586"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given any two $N$-dimensional vectors $\\bx$ and $\\bz$, a function $K(\\bx, \\bz)$ maps them to a scalar value.  \n",
    "A function $K(\\bx, \\bz)$ is called a **valid kernel** if there exist a set of basis $M$ functions $\\phi_i(\\bx)$ such that  \n",
    "$$\n",
    "K(\\bx, \\bz) = \\sum_{i=1}^{M} \\phi_i(\\bx)\\phi_i(\\bz) \\qquad (1)\n",
    "$$\n",
    "\n",
    "Show the following:  \n",
    "\n",
    "1. If $K_1(\\bx, \\bz)$ is a valid kernel, and $f(\\bx)$ is any function that maps $\\bx$ to a scalar, then the kernel $K(\\bx, \\bz) = f(x) K_1(\\bx, \\bz) f(z)$ is also a valid kernel\n",
    "\n",
    "2. If $K_1(\\bx, \\bz)$ and $K_2(\\bx, \\bz)$ are two valid kernels, then the kernel $K(\\bx, \\bz) = K_1(\\bx, \\bz) + K_2(\\bx, \\bz)$ is also a valid kernel\n",
    "\n",
    "3. If $K_1(\\bx, \\bz)$ and $K_2(\\bx, \\bz)$ are two valid kernels, then the kernel $K(\\bx, \\bz) = K_1(\\bx, \\bz) K_2(\\bx, \\bz)$ is also a valid kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. proof:\n",
    "Since $K_1(\\bx, \\bz)$  is a valid kernel, we can easily obtain that\n",
    "\\begin{align*}\n",
    "K(\\bx, \\bz) &= f(x) K_1(\\bx, \\bz) f(z)\\\\\n",
    "&=  f(\\bx)  f(\\bz) \\sum_{i=1}^{M} \\phi_i^1(\\bx)\\phi_i^1(\\bz) \\\\\n",
    "& =  \\sum_{i=1}^{M} \\left( f(\\bx)\\phi_i^1(\\bx)\\right) \\cdot \\left( f(\\bz)\\phi_i^1(\\bz) \\right) \\\\\n",
    "& = \\sum_{i=1}^{M}  \\phi_i(\\bx)\\phi_i(\\bz)\\;,\n",
    "\\end{align*}\n",
    "where $\\phi_i(\\bx) = f(\\bx)\\phi_i^1(\\bx)$ which consists of new set of basis $\\bM'$ functions. Therefore, $K(\\bx, \\bz)$  is a vaild kernel.\n",
    "\n",
    "2. proof:\n",
    "Let $\\phi^1$ and $\\phi^2$ be the feature vectors associated with $K_1(\\bx, \\bz)$ and $K_2(\\bx, \\bz)$ respectively. It is easy to check that \n",
    "\\begin{align*}\n",
    "K(\\bx, \\bz) &= K_1(\\bx, \\bz) + K_2(\\bx, \\bz)\\\\\n",
    "& = \\sum_{i=1}^{M}  \\phi_i^1(\\bx)\\phi_i^1(\\bz) +  \\sum_{i=1}^{N}  \\phi_i^2(\\bx)\\phi_i^2(\\bz) \\\\\n",
    "& =  \\sum_{i=1}^{M}  \\phi_i^1(\\bx)\\phi_i^1(\\bz) + \\phi_i^2(\\bx)\\phi_i^2(\\bz)\\\\\n",
    " & =  \\sum_{i=1}^{M}  \\left(\\phi_i^1(\\bx), \\phi_i^2(\\bx)\\right) \\cdot  \\left(\\phi_i^1(\\bz), \\phi_i^2(\\bz)\\right)\\\\\n",
    " & =   \\sum_{i=1}^{M} \\phi_i(\\bx)\\phi_i(\\bz)\\;,\n",
    "\\end{align*}\n",
    "where $\\phi_i(\\bx) =  \\left(\\phi_i^1(\\bx), \\phi_i^2(\\bx)\\right) $ and $\\phi^2_i(\\bx) = 0, \\forall N < i\\leq M$. Therefore, $K(\\bx, \\bz)$  is a vaild kernel.\n",
    "\n",
    "3. proof:\n",
    "Let $\\phi^1$ and $\\phi^2$ be the feature vectors associated with $K_1(\\bx, \\bz)$ and $K_2(\\bx, \\bz)$ respectively. It is easy to check that \n",
    "\\begin{align*}\n",
    "K(\\bx, \\bz) &= K_1(\\bx, \\bz) K_2(\\bx, \\bz)\\\\\n",
    "&= \\sum_{i=1}^{M}  \\phi_i^1(\\bx)\\phi_i^1(\\bz) \\cdot \\sum_{j=1}^{N}  \\phi_j^2(\\bx)\\phi_j^2(\\bz) \\\\\n",
    "& = \\sum_{i=1}^{MN}  \\phi_i(\\bx)\\phi_i(\\bz)\\;,\n",
    "\\end{align*}\n",
    "where $ \\phi_i(\\bx) =\\phi_i^1(\\bx)\\phi_i^2(\\bx) $, where $i = 1, \\ldots, M$ and $j = 1, \\ldots, N$. Therefore, $K(\\bx, \\bz)$  is a vaild kernel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (70 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Pima Indians Diabetes dataset forecast the occurence of diabetes from eight numerical features.  \n",
    "Metadata for the features are found in the \"README.txt\" file.  \n",
    "Binary logistic regression is used.  \n",
    "\n",
    "**Do the following:**  \n",
    "\n",
    "1. Load and describe the data from file \"Data/PimaIndiansDiabetes/pima-indians-diabetes.csv\".\n",
    "2. Split the data and retain 25% for testing\n",
    "3. Fit a Linear SVM and and a Gaussian RBF SVM model.\n",
    "4. For each model output the in- and out-of-sample confusion matrix, accuracy, precision, recall and F1-score\n",
    "\n",
    "Based on these metrics, can you identify which model performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>glucose</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigre</th>\n",
       "      <th>age</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pregnant     glucose          bp        skin     insulin         bmi  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
       "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
       "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
       "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
       "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
       "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
       "\n",
       "          pedigre         age       label  \n",
       "count  768.000000  768.000000  768.000000  \n",
       "mean     0.471876   33.240885    0.348958  \n",
       "std      0.331329   11.760232    0.476951  \n",
       "min      0.078000   21.000000    0.000000  \n",
       "25%      0.243750   24.000000    0.000000  \n",
       "50%      0.372500   29.000000    0.000000  \n",
       "75%      0.626250   41.000000    1.000000  \n",
       "max      2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset\n",
    "df = pd.read_csv('pima-indians-diabetes.csv') \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indept and dept variables \n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 2)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit linear and rbf SVM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "#==============linear SVM======================\n",
    "# Fitting linear SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier_lsvm = SVC(kernel = 'linear', probability=True,verbose=True, random_state = 0)\n",
    "classifier_lsvm.fit(X_train, y_train)\n",
    "\n",
    "# in sample prediction \n",
    "y_fit_lsvm = classifier_lsvm.predict(X_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_lsvm = classifier_lsvm.predict(X_test)\n",
    "\n",
    "#==============rbf SVM======================\n",
    "# Fitting linear SVM to the Training set\n",
    "classifier_rbfsvm = SVC(kernel = 'rbf', probability=False,verbose=False, random_state = 0)\n",
    "classifier_rbfsvm.fit(X_train, y_train)\n",
    "\n",
    "# in sample prediction \n",
    "y_fit_rbfsvm = classifier_rbfsvm.predict(X_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_rbfsvm = classifier_rbfsvm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM: \n",
      "in-sample confusion matrix\n",
      "act\\pred | class-0  class-1\n",
      "---------------------------\n",
      "class-0  |     319       47\n",
      "class-1  |      83      127\n",
      "\n",
      "out-of-sample confusion matrix\n",
      "act\\pred | class-0  class-1\n",
      "---------------------------\n",
      "class-0  |     120       14\n",
      "class-1  |      31       27\n",
      "\n",
      "RBF SVM: \n",
      "in-sample confusion matrix\n",
      "act\\pred | class-0  class-1\n",
      "---------------------------\n",
      "class-0  |     337       29\n",
      "class-1  |      70      140\n",
      "\n",
      "out-of-sample confusion matrix\n",
      "act\\pred | class-0  class-1\n",
      "---------------------------\n",
      "class-0  |     122       12\n",
      "class-1  |      35       23\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_lsvm_in = confusion_matrix(y_train, y_fit_lsvm)\n",
    "cm_lsvm = confusion_matrix(y_test, y_pred_lsvm)\n",
    "\n",
    "print(\"Linear SVM: \")\n",
    "print(\"in-sample confusion matrix\")\n",
    "print('act\\pred | class-0  class-1')\n",
    "print('---------------------------')\n",
    "print('class-0  |{0:8d} {1:8d}'.format(cm_lsvm_in[0, 0], cm_lsvm_in[0, 1]))\n",
    "print('class-1  |{0:8d} {1:8d}'.format(cm_lsvm_in[1, 0], cm_lsvm_in[1, 1]))\n",
    "\n",
    "print(\"\\nout-of-sample confusion matrix\")\n",
    "print('act\\pred | class-0  class-1')\n",
    "print('---------------------------')\n",
    "print('class-0  |{0:8d} {1:8d}'.format(cm_lsvm[0, 0], cm_lsvm[0, 1]))\n",
    "print('class-1  |{0:8d} {1:8d}'.format(cm_lsvm[1, 0], cm_lsvm[1, 1]))\n",
    "\n",
    "cm_rbfsvm_in = confusion_matrix(y_train, y_fit_rbfsvm)\n",
    "cm_rbfsvm = confusion_matrix(y_test, y_pred_rbfsvm)\n",
    "\n",
    "print(\"\\nRBF SVM: \")\n",
    "print(\"in-sample confusion matrix\")\n",
    "print('act\\pred | class-0  class-1')\n",
    "print('---------------------------')\n",
    "print('class-0  |{0:8d} {1:8d}'.format(cm_rbfsvm_in[0, 0], cm_rbfsvm_in[0, 1]))\n",
    "print('class-1  |{0:8d} {1:8d}'.format(cm_rbfsvm_in[1, 0], cm_rbfsvm_in[1, 1]))\n",
    "\n",
    "print(\"\\nout-of-sample confusion matrix\")\n",
    "print('act\\pred | class-0  class-1')\n",
    "print('---------------------------')\n",
    "print('class-0  |{0:8d} {1:8d}'.format(cm_rbfsvm[0, 0], cm_rbfsvm[0, 1]))\n",
    "print('class-1  |{0:8d} {1:8d}'.format(cm_rbfsvm[1, 0], cm_rbfsvm[1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### commpare Accuracy, Precision, Recall, and F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    in-sample linear SVM  out-sample linear SVM    in-sample rbf SVM   out-sample rbf SVM\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "accuracy        0.774                          0.766                               0.828                          0.755\n",
      "precision        0.730                          0.659                               0.828                          0.657\n",
      "recall              0.605                          0.466                               0.667                         0.397\n",
      "F1                  0.661                          0.545                               0.739                          0.495\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "acc_lsvm_in = accuracy_score(y_train, y_fit_lsvm)\n",
    "acc_rbfsvm_in = accuracy_score(y_train, y_fit_rbfsvm)\n",
    "\n",
    "acc_lsvm = accuracy_score(y_test, y_pred_lsvm)\n",
    "acc_rbfsvm = accuracy_score(y_test, y_pred_rbfsvm)\n",
    "\n",
    "prec_lsvm_in = precision_score(y_train, y_fit_lsvm)\n",
    "prec_rbfsvm_in = precision_score(y_train, y_fit_rbfsvm)\n",
    "\n",
    "prec_lsvm = precision_score(y_test, y_pred_lsvm)\n",
    "prec_rbfsvm = precision_score(y_test, y_pred_rbfsvm)\n",
    "\n",
    "rec_lsvm_in = recall_score(y_train, y_fit_lsvm)\n",
    "rec_rbfsvm_in = recall_score(y_train, y_fit_rbfsvm)\n",
    "\n",
    "rec_lsvm = recall_score(y_test, y_pred_lsvm)\n",
    "rec_rbfsvm = recall_score(y_test, y_pred_rbfsvm)\n",
    "\n",
    "f1_lsvm_in = f1_score(y_train, y_fit_lsvm)\n",
    "f1_rbfsvm_in = f1_score(y_train, y_fit_rbfsvm)\n",
    "\n",
    "f1_lsvm = f1_score(y_test, y_pred_lsvm)\n",
    "f1_rbfsvm = f1_score(y_test, y_pred_rbfsvm)\n",
    "\n",
    "print(\"                    in-sample linear SVM  out-sample linear SVM    in-sample rbf SVM   out-sample rbf SVM\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"accuracy     {0:8.3f}                       {1:8.3f}                            {2:8.3f}                       {3:8.3f}\".format(acc_lsvm_in, acc_lsvm,acc_rbfsvm_in, acc_rbfsvm))\n",
    "print(\"precision     {0:8.3f}                       {1:8.3f}                            {2:8.3f}                       {3:8.3f}\".format(prec_lsvm_in, prec_lsvm,prec_rbfsvm_in, prec_rbfsvm))\n",
    "print(\"recall           {0:8.3f}                       {1:8.3f}                            {2:8.3f}                      {3:8.3f}\".format(rec_lsvm_in, rec_lsvm,rec_rbfsvm_in, rec_rbfsvm))\n",
    "print(\"F1               {0:8.3f}                       {1:8.3f}                            {2:8.3f}                       {3:8.3f}\".format(f1_lsvm_in, f1_lsvm,f1_rbfsvm_in, f1_rbfsvm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on these metrics, can you identify which model performs better?\n",
    "\n",
    "In the showing case, we could see that linear SVM is better performed than rbf SVM since the out sample F1 of linear SVM is slightly higher than rbf SVM. I tried to change $random\\_state$ parameter in $train\\_test\\_split$ and test results for different training test splits. My expectation is rbf SVM will perform better. However, I find that:\n",
    "+ For in sample set, rbf SVM will always perform better than linear SVM, since rbf could provide nonlinear classification which is more powerful. \n",
    "+ For out sample set, the performance of two SVMs will vary based on the different train-test splits.  Sometimes rbf is better, while sometimes linear is better. \n",
    "+ I think the reason is rbf is susceptible to overfitting/training issues  and also the dataset size is too small and therefore, the training set difference will affect the model performance. Sometimes, poor training set will cause rbf overfitting and having large bais. Thus, rbf SVM may perform poorer than linear SVM on out sample set. \n",
    "+ Therefore, we may need larger dataset or tuning hyperparameters for rbf SVM to make it more generalized, accordingly, its performance will be improved. \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

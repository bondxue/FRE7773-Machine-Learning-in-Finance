{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "#### Assigned: 2019-05-09\n",
    "#### Due EoD: 2019-05-22\n",
    "\n",
    "Intraday ETF trading based on 10-sec aggregated equity market data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name:  Mengheng Xue\n",
    "#### NetID: mx586"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPY is an Exchange Traded Fund (ETF) that replicates the S&P 500 index, and trades in exchanges like ordinary equity.  \n",
    "SPY is the most liquid (heavily traded) equity asset in the US.  \n",
    "\n",
    "For all trading days in June 2018 we have aggregated in 10-sec intervals the trading activity in SPY across all exchanges.  \n",
    "The dataset is provided in the file `Resources/Data/spy-10sec-201806.csv` at the class site.   \n",
    "Each row corresponds to a specific 10-sec interval for the corresponding trading day.  \n",
    "A row reports trading activity within the time period that *ends* at the corresponding interval.  \n",
    "\n",
    "Regular trading hours in the US are from 09:30 to 16:00.  \n",
    "You will notice that the 09:30 and 16:00 intervals have much larger traded volume relative to neighboring ones.  \n",
    "This is because the market opens at 09:30 with an opening auction and closes at 16:00 with a closing auction.  \n",
    "The consolidated limit order book is the collection of all quotes to buy (bid) and to sell (ask).  \n",
    "Bid and ask prices/sizes are quotes, i.e. proposals to trade a given quantity at a given price.  \n",
    "VWAP is the volume weighted average of prices at which trades actually occurred.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|Column              | Description  |\n",
    "---------------------|--------------|\n",
    "|volume              | Number of shares traded within the interval  ($\\sum_i V_i $,   summing over all trades $i$)          |\n",
    "|vwap                | Volume Weighted Average Price ( $VWAP = \\sum_i V_i P_i / \\sum_i V_i$,   summing over all trades $i$) |\n",
    "|lowPx/highPx        | Lowest and highest trade prices within the interval                               |\n",
    "|lastBidPx/lastAskPx | Last bid and ask price in the interval                         |\n",
    "|lastBidSz/lastAskSz | Last bid and ask sizes (in hundreds of shares) in the interval |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering\n",
    "\n",
    "Derived quantities of interest are\n",
    "\n",
    "1. The Close Location Value (CLV) indicator. This is an intraday adaptation of a classic technical indicator. It is defined as  \n",
    "$$\n",
    "CLV_t = \\frac{VWAP_t - (lowPx_t + highPx_t)/2}{(highPx_t - lowPx_t)/2}\n",
    "$$\n",
    "It measures the location of the VWAP within interval $t$, relative to the mid-point price between low and high price.  \n",
    "\n",
    "2. The last *quote imbalance* of interval $t$, defined as \n",
    "$$\n",
    "Imbal_t = \\frac{lastBidSz_t - lastAskSz_t}{lastBidSz_t + lastAskSz_t}\n",
    "$$\n",
    "By construction $-1 \\le Imbal \\le 1$.  \n",
    "When $Imbal \\rightarrow 1$, there is much more interest to buy than to sell. \n",
    "Conversely, when $Imbal \\rightarrow -1$ there is much more interest to sell than to buy.  \n",
    "\n",
    "3. The log-transformed volume defined as $logVolume = log10(Volume)$  \n",
    "When working with volume-like quantities (non-negative) taking logs is a common normalization.  \n",
    "Either base 10 or natural logs can be used, base 10 logs may be easier to interpret.\n",
    "\n",
    "We are also interested in the $N$-period forward return in basis points\n",
    "$$\n",
    "\\mathrm{fwdRetNBps}_t = 10000 * \\left(\\frac{VWAP_{t+N}}{VWAP_t} - 1\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem (100 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "+ Construct ML models that use features derived from market observables, to predict price direction in future periods\n",
    "+ Assess the models using 10-sec as well as 1-min aggregation periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation (30 points)\n",
    "+ Load the SPY 10-sec data\n",
    "+ Calculate the CLV and the last quote imbalance for each interval. If highPx is equal to lowPx for an interval, set the CLV value to 0.\n",
    "+ Calculate the 1-period forward VWAP returns in basis points\n",
    "+ Split the dataset into training sample with the first 16 days, and testing sample with the remaining 4 trading days.\n",
    "+ Use the average VWAP in the training set a reference price. Call this $\\mathrm{RefPx}$.\n",
    "+ Compute a cutoff return in basis points as:  $\\mathrm{cutRetBps} = 10000 * (0.02) / \\mathrm{RefPx}$  \n",
    "  This return corresponds to VWAP movement of twice the typical bid-ask spread (i.e 2 * 0.01)\n",
    "+ Add a new column called pxDir1 (price direction) and label the 1-period forward price movement as follows:  \n",
    "  * If fwdRet1Bps > cutRetBps then pxDir1 = +1  \n",
    "  * If abs(fwdRet1Bps) <= cutRetBps then pxDir1 = 0  \n",
    "  * If fwdRet1Bps < - cutRet1Bps then pxDir1 = -1  \n",
    "  Therefore pxDir is a class variable taking values in the set $\\{-1, 0, 1\\}$.\n",
    "+ Re-aggregate the 10-sec data into 1-min data and store them in a new data frame.\n",
    "+ Repeat the process above (CLV, quote imbalance, forward returns, price direction labeling) with the 1-min data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling (60 points)\n",
    "\n",
    "+ Exploratory data analysis (EDA) which should contain:   \n",
    "    * univariate distributions of features: logVolume, clv, imbalance  \n",
    "    * univariate distributions of targets: fwdRet1Bps, pxDir1  \n",
    "    * any other distribution that may reveal a relationship between target and features  \n",
    "    * correlation heat map  \n",
    "+ Construction of a *baseline* model, to be used as a reference.  \n",
    "  The baseline model predicts the price direction class $C=\\{-1, 0, 1\\}$ randomly using the class empirical probability of occurence.  \n",
    "$$\n",
    "\\mathbb{P}(C=\\pm 1) = \\frac{N_{train}(C=\\pm 1)}{N_{train}}, \\quad \n",
    "\\mathbb{P}(C=0) = \\frac{N_{train}(C=0)}{N_{train}}\n",
    "$$\n",
    "Estimate the empirical probabilities of the baseline model using the training set.  \n",
    "Make predictions for pxDir1 (simply sample the multinomial distribution) and use the testing set to report  \n",
    "\n",
    "|Model               | Accuracy  | Precision  | Recall | F1wght | F1micro | \n",
    "---------------------|-----------|------------|--------|---------|---------|\n",
    "|Baseline            | ...       | ...        | ...    | ...     | ...     |\n",
    "\n",
    "Precision, Recall and F1wght should be measured \"weighted\" to account for class occurence and potential imbalance.  \n",
    "F1micro is the \"micro\" F1 score, i.e. it first computes total true/false positives/negatives first and then computes the F1 score.\n",
    "\n",
    "+ Construct *two* models, of which one should be neural net based.  \n",
    "  The other could be any of the classic ML models (Logistic, SVM, Forest, AdaBoost, ...)  \n",
    "  Train and tune the models in order to forecast the target variable pxDir1.  \n",
    "  Evaluate the models on the test sample and add their performance metrics to the table above.  \n",
    "  \n",
    "+ Reaggregate the data using 1-min intervals and repeat the model runs \n",
    "\n",
    "+ Present your conclusions about the best model on the 10-sec and 1-min aggregated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Improvement (10 points)\n",
    "\n",
    "Attempt to improve model performance by introducing one extra feature variable, derived from the existing market data.  \n",
    "The extra variable could be either some kind of moving average or an intraday adaptation of a technical indicator.  \n",
    "Measure the performance improvement for the 10-sec and 1-min dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Follow the submission rules for homeworks.  \n",
    "* The main document is this notebook, describing the methodology and the conclusions.  \n",
    "* Make sure your notebook runs using the standard packages we used in class: numpy/scipy/sklearn/statsmodels/tensorflow/keras.\n",
    "* You can write your own utility classes and functions in separate source code files and import them into this notebook.  \n",
    "* Assume that your separate source code files will be side-by-side with the notebook. \n",
    "* If you are submitting multiple files, put them in a zip archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good luck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline \n",
    "# ====================plot setting ===============\n",
    "pd.options.display.max_colwidth = 70\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 10)\n",
    "plt.rcParams['legend.fontsize'] = 'x-large'\n",
    "plt.rcParams['axes.labelsize'] = 'x-large'\n",
    "plt.rcParams['axes.titlesize'] = 'x-large'\n",
    "plt.rcParams['xtick.labelsize'] = 'x-large'\n",
    "plt.rcParams['ytick.labelsize'] = 'x-large'\n",
    "\n",
    "sn.set_style('darkgrid')\n",
    "sn.set_context('notebook', font_scale=1.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>sym</th>\n",
       "      <th>volume</th>\n",
       "      <th>vwap</th>\n",
       "      <th>lowPx</th>\n",
       "      <th>highPx</th>\n",
       "      <th>lastBidPx</th>\n",
       "      <th>lastAskPx</th>\n",
       "      <th>lastBidSz</th>\n",
       "      <th>lastAskSz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:30:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>395424</td>\n",
       "      <td>272.459140</td>\n",
       "      <td>272.320007</td>\n",
       "      <td>272.489990</td>\n",
       "      <td>272.359985</td>\n",
       "      <td>272.380005</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:30:10</td>\n",
       "      <td>SPY</td>\n",
       "      <td>55692</td>\n",
       "      <td>272.395593</td>\n",
       "      <td>272.339996</td>\n",
       "      <td>272.489990</td>\n",
       "      <td>272.399994</td>\n",
       "      <td>272.410004</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:30:20</td>\n",
       "      <td>SPY</td>\n",
       "      <td>85164</td>\n",
       "      <td>272.443104</td>\n",
       "      <td>272.390015</td>\n",
       "      <td>272.470001</td>\n",
       "      <td>272.450012</td>\n",
       "      <td>272.459991</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:30:30</td>\n",
       "      <td>SPY</td>\n",
       "      <td>26973</td>\n",
       "      <td>272.441112</td>\n",
       "      <td>272.420013</td>\n",
       "      <td>272.459991</td>\n",
       "      <td>272.429993</td>\n",
       "      <td>272.440002</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:30:40</td>\n",
       "      <td>SPY</td>\n",
       "      <td>77809</td>\n",
       "      <td>272.440219</td>\n",
       "      <td>272.410004</td>\n",
       "      <td>272.480011</td>\n",
       "      <td>272.420013</td>\n",
       "      <td>272.429993</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:30:50</td>\n",
       "      <td>SPY</td>\n",
       "      <td>20625</td>\n",
       "      <td>272.439861</td>\n",
       "      <td>272.429993</td>\n",
       "      <td>272.454987</td>\n",
       "      <td>272.440002</td>\n",
       "      <td>272.459991</td>\n",
       "      <td>100</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:31:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>59681</td>\n",
       "      <td>272.446105</td>\n",
       "      <td>272.390015</td>\n",
       "      <td>272.480011</td>\n",
       "      <td>272.410004</td>\n",
       "      <td>272.420013</td>\n",
       "      <td>21</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:31:10</td>\n",
       "      <td>SPY</td>\n",
       "      <td>22027</td>\n",
       "      <td>272.432679</td>\n",
       "      <td>272.399994</td>\n",
       "      <td>272.456696</td>\n",
       "      <td>272.420013</td>\n",
       "      <td>272.429993</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:31:20</td>\n",
       "      <td>SPY</td>\n",
       "      <td>49068</td>\n",
       "      <td>272.442694</td>\n",
       "      <td>272.420013</td>\n",
       "      <td>272.470001</td>\n",
       "      <td>272.429993</td>\n",
       "      <td>272.440002</td>\n",
       "      <td>4</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:31:30</td>\n",
       "      <td>SPY</td>\n",
       "      <td>58974</td>\n",
       "      <td>272.502645</td>\n",
       "      <td>272.440002</td>\n",
       "      <td>272.549988</td>\n",
       "      <td>272.540009</td>\n",
       "      <td>272.549988</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      time  sym  volume        vwap       lowPx      highPx  \\\n",
       "0  2018-06-01  09:30:00  SPY  395424  272.459140  272.320007  272.489990   \n",
       "1  2018-06-01  09:30:10  SPY   55692  272.395593  272.339996  272.489990   \n",
       "2  2018-06-01  09:30:20  SPY   85164  272.443104  272.390015  272.470001   \n",
       "3  2018-06-01  09:30:30  SPY   26973  272.441112  272.420013  272.459991   \n",
       "4  2018-06-01  09:30:40  SPY   77809  272.440219  272.410004  272.480011   \n",
       "5  2018-06-01  09:30:50  SPY   20625  272.439861  272.429993  272.454987   \n",
       "6  2018-06-01  09:31:00  SPY   59681  272.446105  272.390015  272.480011   \n",
       "7  2018-06-01  09:31:10  SPY   22027  272.432679  272.399994  272.456696   \n",
       "8  2018-06-01  09:31:20  SPY   49068  272.442694  272.420013  272.470001   \n",
       "9  2018-06-01  09:31:30  SPY   58974  272.502645  272.440002  272.549988   \n",
       "\n",
       "    lastBidPx   lastAskPx  lastBidSz  lastAskSz  \n",
       "0  272.359985  272.380005         14        100  \n",
       "1  272.399994  272.410004          9         50  \n",
       "2  272.450012  272.459991         18          7  \n",
       "3  272.429993  272.440002          5         20  \n",
       "4  272.420013  272.429993          1         13  \n",
       "5  272.440002  272.459991        100         37  \n",
       "6  272.410004  272.420013         21         50  \n",
       "7  272.420013  272.429993          8         30  \n",
       "8  272.429993  272.440002          4        130  \n",
       "9  272.540009  272.549988         16          4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['date', 'time', 'sym', 'volume', 'vwap', 'lowPx', 'highPx', 'lastBidPx', 'lastAskPx', 'lastBidSz', 'lastAskSz']\n",
    "dataset = pd.read_csv('spy-10sec-201806.csv', skiprows=1, names=names)\n",
    "df_10sec = dataset.copy()\n",
    "df_10sec.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>sym</th>\n",
       "      <th>volume</th>\n",
       "      <th>vwap</th>\n",
       "      <th>lowPx</th>\n",
       "      <th>highPx</th>\n",
       "      <th>lastBidPx</th>\n",
       "      <th>lastAskPx</th>\n",
       "      <th>lastBidSz</th>\n",
       "      <th>lastAskSz</th>\n",
       "      <th>logVolume</th>\n",
       "      <th>clt</th>\n",
       "      <th>Imbal</th>\n",
       "      <th>fwdRet1Bps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:30:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>395424</td>\n",
       "      <td>272.459140</td>\n",
       "      <td>272.320007</td>\n",
       "      <td>272.489990</td>\n",
       "      <td>272.359985</td>\n",
       "      <td>272.380005</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>5.597063</td>\n",
       "      <td>0.637020</td>\n",
       "      <td>-0.754386</td>\n",
       "      <td>-2.332345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:30:10</td>\n",
       "      <td>SPY</td>\n",
       "      <td>55692</td>\n",
       "      <td>272.395593</td>\n",
       "      <td>272.339996</td>\n",
       "      <td>272.489990</td>\n",
       "      <td>272.399994</td>\n",
       "      <td>272.410004</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>4.745793</td>\n",
       "      <td>-0.258679</td>\n",
       "      <td>-0.694915</td>\n",
       "      <td>1.744181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:30:20</td>\n",
       "      <td>SPY</td>\n",
       "      <td>85164</td>\n",
       "      <td>272.443104</td>\n",
       "      <td>272.390015</td>\n",
       "      <td>272.470001</td>\n",
       "      <td>272.450012</td>\n",
       "      <td>272.459991</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>4.930256</td>\n",
       "      <td>0.327454</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>-0.073097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:30:30</td>\n",
       "      <td>SPY</td>\n",
       "      <td>26973</td>\n",
       "      <td>272.441112</td>\n",
       "      <td>272.420013</td>\n",
       "      <td>272.459991</td>\n",
       "      <td>272.429993</td>\n",
       "      <td>272.440002</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>4.430929</td>\n",
       "      <td>0.055529</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-0.032791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:30:40</td>\n",
       "      <td>SPY</td>\n",
       "      <td>77809</td>\n",
       "      <td>272.440219</td>\n",
       "      <td>272.410004</td>\n",
       "      <td>272.480011</td>\n",
       "      <td>272.420013</td>\n",
       "      <td>272.429993</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>4.891030</td>\n",
       "      <td>-0.136794</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.013157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:30:50</td>\n",
       "      <td>SPY</td>\n",
       "      <td>20625</td>\n",
       "      <td>272.439861</td>\n",
       "      <td>272.429993</td>\n",
       "      <td>272.454987</td>\n",
       "      <td>272.440002</td>\n",
       "      <td>272.459991</td>\n",
       "      <td>100</td>\n",
       "      <td>37</td>\n",
       "      <td>4.314394</td>\n",
       "      <td>-0.210373</td>\n",
       "      <td>0.459854</td>\n",
       "      <td>0.229212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:31:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>59681</td>\n",
       "      <td>272.446105</td>\n",
       "      <td>272.390015</td>\n",
       "      <td>272.480011</td>\n",
       "      <td>272.410004</td>\n",
       "      <td>272.420013</td>\n",
       "      <td>21</td>\n",
       "      <td>50</td>\n",
       "      <td>4.775836</td>\n",
       "      <td>0.246509</td>\n",
       "      <td>-0.408451</td>\n",
       "      <td>-0.492796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:31:10</td>\n",
       "      <td>SPY</td>\n",
       "      <td>22027</td>\n",
       "      <td>272.432679</td>\n",
       "      <td>272.399994</td>\n",
       "      <td>272.456696</td>\n",
       "      <td>272.420013</td>\n",
       "      <td>272.429993</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>4.342955</td>\n",
       "      <td>0.152888</td>\n",
       "      <td>-0.578947</td>\n",
       "      <td>0.367606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:31:20</td>\n",
       "      <td>SPY</td>\n",
       "      <td>49068</td>\n",
       "      <td>272.442694</td>\n",
       "      <td>272.420013</td>\n",
       "      <td>272.470001</td>\n",
       "      <td>272.429993</td>\n",
       "      <td>272.440002</td>\n",
       "      <td>4</td>\n",
       "      <td>130</td>\n",
       "      <td>4.690798</td>\n",
       "      <td>-0.092555</td>\n",
       "      <td>-0.940299</td>\n",
       "      <td>2.200507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>09:31:30</td>\n",
       "      <td>SPY</td>\n",
       "      <td>58974</td>\n",
       "      <td>272.502645</td>\n",
       "      <td>272.440002</td>\n",
       "      <td>272.549988</td>\n",
       "      <td>272.540009</td>\n",
       "      <td>272.549988</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>4.770661</td>\n",
       "      <td>0.139112</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.852283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      time  sym  volume        vwap       lowPx      highPx  \\\n",
       "0  2018-06-01  09:30:00  SPY  395424  272.459140  272.320007  272.489990   \n",
       "1  2018-06-01  09:30:10  SPY   55692  272.395593  272.339996  272.489990   \n",
       "2  2018-06-01  09:30:20  SPY   85164  272.443104  272.390015  272.470001   \n",
       "3  2018-06-01  09:30:30  SPY   26973  272.441112  272.420013  272.459991   \n",
       "4  2018-06-01  09:30:40  SPY   77809  272.440219  272.410004  272.480011   \n",
       "5  2018-06-01  09:30:50  SPY   20625  272.439861  272.429993  272.454987   \n",
       "6  2018-06-01  09:31:00  SPY   59681  272.446105  272.390015  272.480011   \n",
       "7  2018-06-01  09:31:10  SPY   22027  272.432679  272.399994  272.456696   \n",
       "8  2018-06-01  09:31:20  SPY   49068  272.442694  272.420013  272.470001   \n",
       "9  2018-06-01  09:31:30  SPY   58974  272.502645  272.440002  272.549988   \n",
       "\n",
       "    lastBidPx   lastAskPx  lastBidSz  lastAskSz  logVolume       clt  \\\n",
       "0  272.359985  272.380005         14        100   5.597063  0.637020   \n",
       "1  272.399994  272.410004          9         50   4.745793 -0.258679   \n",
       "2  272.450012  272.459991         18          7   4.930256  0.327454   \n",
       "3  272.429993  272.440002          5         20   4.430929  0.055529   \n",
       "4  272.420013  272.429993          1         13   4.891030 -0.136794   \n",
       "5  272.440002  272.459991        100         37   4.314394 -0.210373   \n",
       "6  272.410004  272.420013         21         50   4.775836  0.246509   \n",
       "7  272.420013  272.429993          8         30   4.342955  0.152888   \n",
       "8  272.429993  272.440002          4        130   4.690798 -0.092555   \n",
       "9  272.540009  272.549988         16          4   4.770661  0.139112   \n",
       "\n",
       "      Imbal  fwdRet1Bps  \n",
       "0 -0.754386   -2.332345  \n",
       "1 -0.694915    1.744181  \n",
       "2  0.440000   -0.073097  \n",
       "3 -0.600000   -0.032791  \n",
       "4 -0.857143   -0.013157  \n",
       "5  0.459854    0.229212  \n",
       "6 -0.408451   -0.492796  \n",
       "7 -0.578947    0.367606  \n",
       "8 -0.940299    2.200507  \n",
       "9  0.600000    0.852283  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# funciton to calculate Close Location Value\n",
    "def calculate_clt(vwap, lowPx, highPx):\n",
    "    if lowPx == highPx:\n",
    "        clt = 0\n",
    "    else:\n",
    "         clt = (vwap-(lowPx+highPx)/2) / ((highPx-lowPx)/2)\n",
    "    return clt\n",
    "\n",
    "# function to calculate last quote imbalance\n",
    "def calculate_Imbal(lastBidSz, lastAskSz):\n",
    "    imbal = (lastBidSz - lastAskSz)/(lastBidSz+lastAskSz)\n",
    "    return imbal\n",
    "\n",
    "# function to calculate N-period forward return in basis points, default is 1-period \n",
    "def calculate_fwdRetNBps(df, period=1):\n",
    "    for i in range(0, len(df)-period):\n",
    "        df.loc[i, 'fwdRet1Bps'] = 10000 * (df.loc[i+period, 'vwap']/df.loc[i, 'vwap'] - 1)\n",
    "    return df \n",
    "\n",
    "# function to calcuate log10 volume\n",
    "def calculate_logVolume(volume):\n",
    "    logVolume = np.log10(volume)\n",
    "    return logVolume\n",
    "\n",
    "# function to update clt, lmbal, logVolume and fwdRetNBps of dataframe \n",
    "def update_features(df):\n",
    "    df['logVolume'] = df.apply(lambda x: calculate_logVolume(x.volume), axis=1)\n",
    "    df['clt'] = df.apply(lambda x: calculate_clt(x.vwap, x.lowPx, x.highPx), axis=1)\n",
    "    df['Imbal'] = df.apply(lambda x: calculate_Imbal(x.lastBidSz, x.lastAskSz), axis=1)\n",
    "    df = calculate_fwdRetNBps(df)\n",
    "    return df\n",
    "\n",
    "df_10sec = update_features(df_10sec)   \n",
    "df_10sec.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengh of dataset: 46815\n",
      "lengh of training set: 37451\n",
      "lengh of testing set: 9364\n"
     ]
    }
   ],
   "source": [
    "# function calculate price direction values {-1, 0, 1}\n",
    "def calculate_pxDir1(fwdRet1Bps, cutRetBps):\n",
    "    if fwdRet1Bps > cutRetBps:\n",
    "        pxDir1 = 1\n",
    "    elif abs(fwdRet1Bps) <= cutRetBps:\n",
    "        pxDir1 = 0\n",
    "    else:\n",
    "        pxDir1 = -1\n",
    "    return pxDir1\n",
    "\n",
    "# function to separate dataset into 16 days training set and 4 days test set\n",
    "def train_test_split(df):\n",
    "    train_mask = (df['date'] <= '2018-06-22')\n",
    "    train_set = df.loc[train_mask]\n",
    "    RefPx = train_set['vwap'].mean()  # reference price\n",
    "    cutRetBps = 10000*0.02/RefPx # cutoff return \n",
    "    df['pxDir1'] = df.apply(lambda x: calculate_pxDir1(x.fwdRet1Bps, cutRetBps), axis=1)\n",
    "    train_mask = (df['date'] <= '2018-06-22') \n",
    "    train_set = df.loc[train_mask]\n",
    "    test_mask = (df['date'] >= '2018-06-25')\n",
    "    test_set = df.loc[test_mask]\n",
    "    test_set = test_set.reset_index(drop=True)\n",
    "    \n",
    "    return df, train_set, test_set\n",
    "\n",
    "df_10s, train_set10s, test_set10s = train_test_split(df_10sec)\n",
    "\n",
    "print('lengh of dataset: {}'.format(len(df_10s)))\n",
    "print('lengh of training set: {}'.format(len(train_set10s)))\n",
    "print('lengh of testing set: {}'.format(len(test_set10s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = ['logVolume', 'clt', 'Imbal']\n",
    "df_10sec[features].plot(kind='density', subplots=True, layout=(1,3), sharex=False)\n",
    "plt.show()\n",
    "\n",
    "targets = ['fwdRet1Bps', 'pxDir1']\n",
    "df_10sec[targets].plot(kind='density', subplots=True, layout=(1,3), sharex=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a7158dc369c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mX_train10s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train10s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test10s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test10s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set10s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set10s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "\n",
    "def data_prerocess(train_set, test_set):\n",
    "    X_train = train_set.iloc[:, 11:14].values # logVolume, clt, Imbal\n",
    "    y_train = train_set.iloc[:, -1].values # pxDir1\n",
    "    X_test = test_set.iloc[:, 11:14].values  \n",
    "    y_test = test_set.iloc[:, -1].value\n",
    "    \n",
    "    # Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \n",
    "    # Transform {-1, 0, 1}  into 3 categories \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train10s, y_train10s, X_test10s, y_test10s = data_preprocess(train_set10s, test_set10s)\n",
    "    \n",
    "\n",
    "# print(y_test_10sec)\n",
    "\n",
    "def built_model()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim = 3 , activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "# model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(X_train_10sec, y_train_10sec, epochs = 10, batch_size = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# scores = model.evaluate(X_test_10sec , y_test_10sec)\n",
    "# print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# # Feature Scaling\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X_train_10sec = sc.fit_transform(X_train_10sec)\n",
    "# X_test_10sec = sc.transform(X_test_10sec)\n",
    "\n",
    "\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # Initialising the ANN\n",
    "# classifier = Sequential()\n",
    "\n",
    "# # Adding the input layer and the first hidden layer\n",
    "# classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 4))\n",
    "\n",
    "# # Adding the second hidden layer\n",
    "# classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# # Adding the output layer\n",
    "# classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation='softmax'))\n",
    "\n",
    "# # Compiling the ANN\n",
    "# classifier.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# # Fitting the ANN to the Training set\n",
    "# classifier.fit(X_train_10sec, y_train_10sec, batch_size = 10, epochs = 100)\n",
    "\n",
    "# # Predicting the Test set results\n",
    "# y_pred = classifier.predict(X_test_10sec)\n",
    "# y_pred\n",
    "\n",
    "\n",
    "# # Feature Scaling\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "\n",
    "# # Feature Scaling\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "# train = scaler.fit_transform(trainset10sec_raw)\n",
    "# test = scaler.fit_transform(testset10sec_raw)\n",
    "# print(\"Number of entries (training set, test set): \" + str((len(train), len(test))))\n",
    "\n",
    "# # function to create train and test set\n",
    "# def create_dataset(dataset, train_window_size=16, test_window_size=1):\n",
    "#     data_X, data_Y = [], []\n",
    "#     for i in range(len(dataset) - train_window_size-test_window_size + 1):\n",
    "#         a = dataset[i:(i + train_window_size), 0]\n",
    "#         b = dataset[(i+train_window_size):(i+train_window_size+test_window_size), 0]\n",
    "#         data_X.append(a)\n",
    "#         data_Y.append(b)\n",
    "#     return(np.array(data_X), np.array(data_Y))\n",
    "\n",
    "# train_X, train_y = create_dataset(train)\n",
    "# test_X, test_y = create_dataset(test)\n",
    "# print(\"Original training data shape:\")\n",
    "# print(train_X.shape)\n",
    "# print(train_y.shape)\n",
    "\n",
    "# # Reshape the input data into appropriate form for Keras.\n",
    "# train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 1))\n",
    "# test_X = np.reshape(test_X, (test_X.shape[0], test_X.shape[1], 1))\n",
    "# print(\"New training data shape:\")\n",
    "# print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "predictions = model.predict(X_test_10sec)\n",
    "prediction_ = np.argmax(predictions, axis = 1) \n",
    "prediction_ = encoder.inverse_transform(prediction_) \n",
    "print(X_test_10sec)\n",
    "\n",
    "# print(prediction_)\n",
    "\n",
    "for i in range(len(prediction_ )):\n",
    "    print(prediction_[i])\n",
    "    \n",
    "print(type(y_test_10sec))\n",
    "precision = precision_score(y_test_10sec, prediction_, average=None)\n",
    "# recall = recall_score(y_test_10sec, prediction_)\n",
    "accuracy = accuracy_score(y_test_10sec, prediction_)\n",
    "# f1_wght = f1_score(y_test_10sec, prediction_, average='weighted')\n",
    "# f1_micro = f1_score(y_test_10sec, prediction_, average='micro')\n",
    "print(precision)\n",
    "# print(recall)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "prior_pos = len(train_set_10sec[train_set_10sec['pxDir1']==1]) / len(train_set_10sec)\n",
    "prior_neg = len(train_set_10sec[train_set_10sec['pxDir1']==-1]) / len(train_set_10sec)\n",
    "prior_neu = len(train_set_10sec[train_set_10sec['pxDir1']==0]) / len(train_set_10sec)\n",
    "\n",
    "# y = np.array([-1, 0, 1])\n",
    "# X = train_set_10sec['pxDir1']\n",
    "# clf = MultinomialNB(class_prior=[prior_neg, prior_neu, prior_pos])\n",
    "# model = clf.fit(X, y)\n",
    "\n",
    "# def bulid_baseline_model():\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(window_size = 16):\n",
    "    model = Sequential() \n",
    "    model.add(LSTM(input_shape = (window_size, 4), units=80, return_sequences = True, activation='relu')) # add first LSTM layer \n",
    "    model.add(Dropout(0.2))# dropout regulization to avoid overfitting\n",
    "    model.add(LSTM(input_shape=(window_size, 4), units=80, return_sequences = False, activation='relu')) # add second LSTM layer \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 3, kernel_initializer = 'uniform', activation='softmax'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and fit \n",
    "def fit_model(model, train_X, train_y):\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, mode='auto', baseline=None)\n",
    "    model.fit( train_X, train_y, batch_size=50, epochs=100, validation_split=0.1, callbacks=[es], verbose=1)\n",
    "    return model\n",
    "    \n",
    "model_fit = fit_model(model, train_X, train_y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(X_train, y_train, X_test, y_test, classifier):\n",
    "    columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1wght', 'F1micro']    \n",
    "    model_name = str(type(classifier).__name__)\n",
    "    \n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1_wght = f1_score(y_test, predictions, average='weighted')\n",
    "    f1_micro = f1_score(y_test, predictions, average='micro')\n",
    "    \n",
    "    metrics = [model_name, accuracy, precision, recall, f1_wght, f1_micro]\n",
    "    df_report = pd.DataFrame(metrics, columns=columns)\n",
    "    return df_report\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1min = dataset.copy()\n",
    "df_1min['dateTime'] =  pd.to_datetime(df_1min['date' ] + ' ' + df_1min['time'])\n",
    "df_1min.set_index('dateTime', inplace=True)\n",
    "df_1min = df_1min.resample('1T').sum() # re-aggregate 10-sec data into 1-min data \n",
    "df_1min = df_1min[df_1min.index.weekday<5] # filter the weekend \n",
    "df_1min = df_1min.reset_index()\n",
    "df_1min['date'] = df_1min['dateTime'].dt.strftime('%d/%m/%Y')\n",
    "df_1min['time'] = df_1min['dateTime'].dt.strftime('%H:%M:%S')\n",
    "time_mask = (df_1min['time']>='09:30:00') & (df_1min['time']<='16:00:00') # filter not the daily trading time \n",
    "df_1min = df_1min.loc[time_mask].drop(['dateTime'], axis=1)\n",
    "df_1min = df_1min.reset_index(drop=True)\n",
    "df_1min['sym'] = 'SPY' # add back sym \n",
    "df_1min.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1min = update_features(df_1min)   \n",
    "df_1min, train_set_1min, test_set_1min = train_test_split(df_1min)\n",
    "cols = df_10sec.columns.tolist()\n",
    "df_1min = df_1min[cols]\n",
    "df_1min.head(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "262.667px",
    "left": "313px",
    "right": "20px",
    "top": "123px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

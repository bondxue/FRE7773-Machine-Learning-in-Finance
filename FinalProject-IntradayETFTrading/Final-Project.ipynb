{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "#### Assigned: 2019-05-09\n",
    "#### Due EoD: 2019-05-22\n",
    "\n",
    "Intraday ETF trading based on 10-sec aggregated equity market data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name:  <...>\n",
    "#### NetID: <...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPY is an Exchange Traded Fund (ETF) that replicates the S&P 500 index, and trades in exchanges like ordinary equity.  \n",
    "SPY is the most liquid (heavily traded) equity asset in the US.  \n",
    "\n",
    "For all trading days in June 2018 we have aggregated in 10-sec intervals the trading activity in SPY across all exchanges.  \n",
    "The dataset is provided in the file `Resources/Data/spy-10sec-201806.csv` at the class site.   \n",
    "Each row corresponds to a specific 10-sec interval for the corresponding trading day.  \n",
    "A row reports trading activity within the time period that *ends* at the corresponding interval.  \n",
    "\n",
    "Regular trading hours in the US are from 09:30 to 16:00.  \n",
    "You will notice that the 09:30 and 16:00 intervals have much larger traded volume relative to neighboring ones.  \n",
    "This is because the market opens at 09:30 with an opening auction and closes at 16:00 with a closing auction.  \n",
    "The consolidated limit order book is the collection of all quotes to buy (bid) and to sell (ask).  \n",
    "Bid and ask prices/sizes are quotes, i.e. proposals to trade a given quantity at a given price.  \n",
    "VWAP is the volume weighted average of prices at which trades actually occurred.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|Column              | Description  |\n",
    "---------------------|--------------|\n",
    "|volume              | Number of shares traded within the interval  ($\\sum_i V_i $,   summing over all trades $i$)          |\n",
    "|vwap                | Volume Weighted Average Price ( $VWAP = \\sum_i V_i P_i / \\sum_i V_i$,   summing over all trades $i$) |\n",
    "|lowPx/highPx        | Lowest and highest trade prices within the interval                               |\n",
    "|lastBidPx/lastAskPx | Last bid and ask price in the interval                         |\n",
    "|lastBidSz/lastAskSz | Last bid and ask sizes (in hundreds of shares) in the interval |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering\n",
    "\n",
    "Derived quantities of interest are\n",
    "\n",
    "1. The Close Location Value (CLV) indicator. This is an intraday adaptation of a classic technical indicator. It is defined as  \n",
    "$$\n",
    "CLV_t = \\frac{VWAP_t - (lowPx_t + highPx_t)/2}{(highPx_t - lowPx_t)/2}\n",
    "$$\n",
    "It measures the location of the VWAP within interval $t$, relative to the mid-point price between low and high price.  \n",
    "\n",
    "2. The last *quote imbalance* of interval $t$, defined as \n",
    "$$\n",
    "Imbal_t = \\frac{lastBidSz_t - lastAskSz_t}{lastBidSz_t + lastAskSz_t}\n",
    "$$\n",
    "By construction $-1 \\le Imbal \\le 1$.  \n",
    "When $Imbal \\rightarrow 1$, there is much more interest to buy than to sell. \n",
    "Conversely, when $Imbal \\rightarrow -1$ there is much more interest to sell than to buy.  \n",
    "\n",
    "3. The log-transformed volume defined as $logVolume = log10(Volume)$  \n",
    "When working with volume-like quantities (non-negative) taking logs is a common normalization.  \n",
    "Either base 10 or natural logs can be used, base 10 logs may be easier to interpret.\n",
    "\n",
    "We are also interested in the $N$-period forward return in basis points\n",
    "$$\n",
    "\\mathrm{fwdRetNBps}_t = 10000 * \\left(\\frac{VWAP_{t+N}}{VWAP_t} - 1\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem (100 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "+ Construct ML models that use features derived from market observables, to predict price direction in future periods\n",
    "+ Assess the models using 10-sec as well as 1-min aggregation periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation (30 points)\n",
    "+ Load the SPY 10-sec data\n",
    "+ Calculate the CLV and the last quote imbalance for each interval. If highPx is equal to lowPx for an interval, set the CLV value to 0.\n",
    "+ Calculate the 1-period forward VWAP returns in basis points\n",
    "+ Split the dataset into training sample with the first 16 days, and testing sample with the remaining 4 trading days.\n",
    "+ Use the average VWAP in the training set a reference price. Call this $\\mathrm{RefPx}$.\n",
    "+ Compute a cutoff return in basis points as:  $\\mathrm{cutRetBps} = 10000 * (0.02) / \\mathrm{RefPx}$  \n",
    "  This return corresponds to VWAP movement of twice the typical bid-ask spread (i.e 2 * 0.01)\n",
    "+ Add a new column called pxDir1 (price direction) and label the 1-period forward price movement as follows:  \n",
    "  * If fwdRet1Bps > cutRetBps then pxDir1 = +1  \n",
    "  * If abs(fwdRet1Bps) <= cutRetBps then pxDir1 = 0  \n",
    "  * If fwdRet1Bps < - cutRet1Bps then pxDir1 = -1  \n",
    "  Therefore pxDir is a class variable taking values in the set $\\{-1, 0, 1\\}$.\n",
    "+ Re-aggregate the 10-sec data into 1-min data and store them in a new data frame.\n",
    "+ Repeat the process above (CLV, quote imbalance, forward returns, price direction labeling) with the 1-min data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling (60 points)\n",
    "\n",
    "+ Exploratory data analysis (EDA) which should contain:   \n",
    "    * univariate distributions of features: logVolume, clv, imbalance  \n",
    "    * univariate distributions of targets: fwdRet1Bps, pxDir1  \n",
    "    * any other distribution that may reveal a relationship between target and features  \n",
    "    * correlation heat map  \n",
    "+ Construction of a *baseline* model, to be used as a reference.  \n",
    "  The baseline model predicts the price direction class $C=\\{-1, 0, 1\\}$ randomly using the class empirical probability of occurence.  \n",
    "$$\n",
    "\\mathbb{P}(C=\\pm 1) = \\frac{N_{train}(C=\\pm 1)}{N_{train}}, \\quad \n",
    "\\mathbb{P}(C=0) = \\frac{N_{train}(C=0)}{N_{train}}\n",
    "$$\n",
    "Estimate the empirical probabilities of the baseline model using the training set.  \n",
    "Make predictions for pxDir1 (simply sample the multinomial distribution) and use the testing set to report  \n",
    "\n",
    "|Model               | Accuracy  | Precision  | Recall | F1wght | F1micro | \n",
    "---------------------|-----------|------------|--------|---------|---------|\n",
    "|Baseline            | ...       | ...        | ...    | ...     | ...     |\n",
    "\n",
    "Precision, Recall and F1wght should be measured \"weighted\" to account for class occurence and potential imbalance.  \n",
    "F1micro is the \"micro\" F1 score, i.e. it first computes total true/false positives/negatives first and then computes the F1 score.\n",
    "\n",
    "+ Construct *two* models, of which one should be neural net based.  \n",
    "  The other could be any of the classic ML models (Logistic, SVM, Forest, AdaBoost, ...)  \n",
    "  Train and tune the models in order to forecast the target variable pxDir1.  \n",
    "  Evaluate the models on the test sample and add their performance metrics to the table above.  \n",
    "  \n",
    "+ Reaggregate the data using 1-min intervals and repeat the model runs \n",
    "\n",
    "+ Present your conclusions about the best model on the 10-sec and 1-min aggregated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Improvement (10 points)\n",
    "\n",
    "Attempt to improve model performance by introducing one extra feature variable, derived from the existing market data.  \n",
    "The extra variable could be either some kind of moving average or an intraday adaptation of a technical indicator.  \n",
    "Measure the performance improvement for the 10-sec and 1-min dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Follow the submission rules for homeworks.  \n",
    "* The main document is this notebook, describing the methodology and the conclusions.  \n",
    "* Make sure your notebook runs using the standard packages we used in class: numpy/scipy/sklearn/statsmodels/tensorflow/keras.\n",
    "* You can write your own utility classes and functions in separate source code files and import them into this notebook.  \n",
    "* Assume that your separate source code files will be side-by-side with the notebook. \n",
    "* If you are submitting multiple files, put them in a zip archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good luck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
